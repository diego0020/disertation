%Introduction

The human brain can be analyzed from several different points of views. Some disciplines look at it from the outside, and try to understand how it responds to stimuli, how it behaves on different conditions, how it adapts to new contexts and how it changes over time. Some disciplines look at it at cellular and molecular level, trying to understand the chemical reactions that go on inside each cell making it work. Others analyze the activity of a groups of cells analyzing how they cooperate. Some focus on larger structures composed of several cells, trying to understand how they connect to each other and how the different types of organs complement each other. It is of special interest seeing how it matures over time, and how it recovers from traumatic events. Additionally it is important to analyze how it degenerates over time, and what diseases can affect it in order to create treatments and help the brain heal. 
These tasks are carried out by different specialists in very different environments. There are also a wide range of tools designed for studying the brain, from microscopes and voltage clamping techniques, to psychology instruments. There are also studies based on animals with similar structures or even electronics circuits specifically built to emulate the human brain. 

This shows that understanding the brain is far from easy, and that it involves an enormous amount of skills, tools and knowledge. It can also be seen that all of these information gathered from different perspectives must be integrated in other to get a full understanding. This will require teams from diverse specialties and contexts to work together, each providing one piece of the puzzle. 

This task will also require support from computational tools in order to be efficient. These tools should allow experts from different contexts to be productive as teams and to integrate data acquired using a wide array of methods. Data itself will be highly heterogeneous and there will be lots of it. As tools advance we become more efficient at making experiments and generating data, and the bottleneck has become analyzing it. In other words, data is acquired at a fastest rate than it can be understood. 

This is indeed a complex scenario, and it is evident that a single software tool will not be able to solve all the problems. Tackling the whole problem at once is also an impossible tasks. We need to break down the problem into simpler tasks that can be attacked without loosing the big picture. Doing this analysis is itself a challenging task, that can't be addressed lightly. Fortunately this kind of problems are found in several business applications and  software engineering techniques that can manage them have been developed. 

As mentioned previously, the problem of integrating brain data involves several points of view and several types of work-flows. This is a clear signal of the need for different applications instead of a single, all-mighty application. Nevertheless these applications will likely share several aspects. Analyzing these commonalities and differences is the first step towards a solution to the problem. 
Methodologies to do this can be borrowed from software product line engineering \autocite{pohl_software_2005}, model driven software engineering \autocite{brambilla_model-driven_2012} and generative programming \autocite{czarnecki_generative_2000}. 

The first task will be the scoping of the domain and the selection of a domain where a it is feasible to make a contribution. Afterwards the commonalities and differences in the needs of the different stake-holders, work-flows, and data in the selected domain are analyzed in order to create a feature model. This model will be the basis for a proposal of an applications family, where each application addresses a particular problem inside the domain. 


\section{Domain Engineering}

Domain Engineering is the practice of selecting and characterizing the domain in which the application family will be built. A good description of the steps required for this task can be found in \autocite{czarnecki_generative_2000}. We will follow these steps in order to identify the domain where we will work on the rest of the thesis.

\subsection{Domain Scoping}

As noted previously brain research involves several specialties, skills and techniques. It could be argued that because the brain is involved in almost every human action, all human and social sciences are at the end studying the brain. However in this project we will focus on more direct studies of the human brain. In particular we want to analyze its physical structure. Under this condition there are still a broad ways of looking at it.

In domain engineering the decision of where to focus must be also influenced by the strengths and experience of the organization, in this case our research group. Previous projects of the group (\footnote{Proyectos de Darwin, Jaime, Marcela}) have principally focused on analysis of medical images at m.m. scale, acquired by CT and MRI machines. We have good relationships with radiology departments at several hospitals and therefore access to images and, more critical, domain experts. MRI is an specially interesting technique as it does not produce ionizing radiation, and therefore is harmless for the subject. While CT and general x-rays involve radiating the human body. This is not harmful at small doses but the effects may accumulate over time. Therefore these techniques must be used with care and only when there is a valid medical reason that justifies it. On the other side, MRI scans can be applied to any subject and there is no need for a medical justification (but probably the study must be approved by an ethical committee). MRI scanners are also versatile machines which can acquire numerous kinds of images. Structural images can be acquired at different configurations which provides better contrast for different tissues or molecules. Advanced techniques grant the ability to do spectroscopy in order to characterize the composition of specific areas of the brain. By using contrast agents it is also possible to precisely locate specific proteins, cells or structures. 
For these reasons we chose to focus our development on images acquired by MRI. We also chose to focus on T1 and T2 weighted structural images, Diffusion weighted images and BOLD f-MRI. This decision was also caused by the previous experiences in the group, but nevertheless keep in mind that domain definition and scoping is an iterative process, and therefore this decision will certainly be revisited in the future.

An objective of the project from the start has been the integration of data, therefore even though we are focusing on MRI data, we need additional data to provide context and therefore a more complete picture. Recall that one of our hypotheses is that the brain is better understood by teams of specialists who can bring different perspectives. One of our challenges was linking structure and function of the brain. This function of the brain may signify quick reactions to stimuli, like for example catching a ball, all the way to complex social behaviors over several years. This kind of data can be collected by economists, psychologists, epidemiologists, and sociologists among others. This information can be very complex, but a non trivial subset of it consists of numerical, nominal and ordinal variables which can be registered in spreadsheet tables. We will attempt to integrate data from these diverse set of disciplines if it is presented in a table-like format, but we will not try to interpret the data in any way. This responsibility will fall on end users. 

Finally there is another important kind of data we want to consider as it provides a perfect link between structure and functioning of the brain. This is, data from TMS exams. We are very lucky to have access to neurophysiologists specialized in this kind of exams, which can further enlighten the functioning of the brain and its relationship to its structure. To recapitulate, the current project is going to consider the following kinds of data:

\begin{itemize}
\item MRI brain images
\begin{itemize}
\item Structural T1 and T2 weighted
\item Diffusion Weighted Images
\item Functional MRI
\end{itemize}
\item TMS exams
\item Tabular data from other disciplines
\begin{itemize}
\item Nominal variables
\item Ordinal variables
\item Numeric variables
\end{itemize}
\end{itemize}  

Another key aspect of domain scoping is identifying stakeholders and their interests. As mentioned earlier the brain can be studied from a clinical perspective with the aim of healing it. This is usually done case by case in hospitals or health centers. While this is a very important activity, we are most interested in analyzing cohorts of subjects. Also, getting into the clinical diagnosis practice would involve dealing with significant more regulations, which would increase the complexity of the project. The other major group of users of medical images are brain researchers. As was described in the introduction chapter, this is our main target. In particular we focus on interdisciplinary brain research projects, where there is data from multiple subjects and different nature, and an interest to find relationships across the different dimensions. The data available for each subject should be reasonably consistent across the complete sample. The stakeholders are therefore the researchers involved in such a project. These researchers can come from several backgrounds, which include

\begin{itemize}
\item Radiologists
\item Physicians
\item Psychologists
\item Physiologists
\item Epidemiologists
\item Economists
\end{itemize}

Their goals are extracting information and meaning out of the data. As mentioned earlier this is the core problem that lead to the birth of visual analytics. However it is crucial to understand that each specialist will inevitably have his own interests and his own methods. The real challenge is creating a common framework that would help and encourage specialists from different disciplines to work together and collaborate efficiently. Therefore in order to achieve the ultimate goal it is necessary to improve communication inside the team, and to encourage specialists to go out of their zone of comfort and ask questions about data they don't usually look at. However this must be done responsibly, the objective is not to ignore the true specialists in a particular kind of data, but the opposite, improve communication between these specialists. 

Each specialty also incorporates their own work-flows. These include protocols for data acquisition, pre-processing, storage, processing and analysis. As mentioned in the introduction analysis usually take the form of null hypothesis significance testing, and plenty of tools exist for this purpose. Nonetheless exploratory data analysis is also very important, specially when huge amounts of data are available. Traditionally data acquisition was also tuned for the testing of particular hypotheses, but it is becoming common to acquire data in a more open-ended fashion. In this project we will not deal with data acquisition nor data processing. The focus is on visual exploratory analysis of already processed data.  

In conclusion, the scope of the project is now bounded in data types (MRI and tabular), stakeholders (interdisciplinary research groups), and tasks (visual exploratory analysis). However, it is worth reiterating that domain scoping is an iterative process, which has to go on for the duration of the project. 

Domain Model

%- Stakeholders
%- Users
%-- points of view
%-- activities
%-- teams
%-- selfish
%
%-- Alternatives
%-- choose
%- Scope
%-- Alternatives
%-- Choose


\subsection{Domain Modeling}

After choosing and bounding the domain we need to go deeper into its characterization. This task is accomplished by reading bibliography in the domain, contacting stakeholders and analyzing existing software. The details of the information gathered at this stage can be found in chapter \ref{chap_related}. In this section we will describe the domain based on that information.

%Workflow
A typical MRI experiment starts with a set of hypotheses which want to be tested on a target population. A protocol is designed to test the hypotheses and if possible correct sample sizes are calculated. Afterwards participants are recruited, and data is collected. This process is not always smooth, and often corrections must be made and acquisition repeated. Inevitable some data will not be useful and some participants will have to be removed from the study. Data acquired from the MRI machine must be taken out, stored and processed. There are different processing pipelines for different kinds of images, but most of them are designed to perform statistical tests using the images and some external variables. The results of the statistical tests have to be interpreted in order to write the report of the experiment and what we learned from it.

As mentioned in chapter \ref{chap_intro} we intend to use data collected in these experiments as well as data collected in open-ended fashion to perform exploratory and data-driven analyzes. Still, most of the steps in traditional experiments will be the same, and we are required to play nice with existing tools and work-flows. 

\begin{figure}

\caption{\label{fig_workflows}}
\end{figure}

Figure \ref{fig_workflows} shows the comparison of the two work-flows. It can be seen that the input for traditional experiments are hypotheses, while the output of data driven research are also hypotheses. Meanwhile the input to the data-driven research loop is data, which as mentioned earlier can come from finished experiments. This shows how both work-flows can complement each other. Also notice data processing takes place at both loops. The meaning is that the data-driven and hypotheses-driven research domains are deeply linked.  

%Data
\begin{figure}

\caption{\label{fig_datum_class}}
\end{figure}

Data is the most important object in our domain, so we need to characterize it in a more precise way. Inside our scope, data is a collection of data-points each of whom is also called a datum. Figure \ref{fig_datum_class} shows a class diagram of a datum. The highlights of the drawing are

%definitions

\begin{itemize}
\item Each datum is associated to an Entity
\item An entity may be a single subject or a sample
\item A sample is composed of several subjects, each subject can belong to several samples
\item The two main kind of datum (in our scope) are spatial and variable values
\item An spatial datum may be an image or polydata
\item All spatial objects are associated with a coordinates system
\item Transforms can be used to map between two coordinate systems
\item Images are composed of voxels
\item Polydata are collections of vertices, edges and polygons
\item Voxels, edges, vertices and polygons may be associated with one or more scalar values.
\item All spatial data must me associated to some meta-data
\item VariableValues belong to a certain variable
\item Each variable is associated with meta-data  
\item Data which is neither a VariableValue nor a spatial object can be encoded into textual annotations
\end{itemize} 

Meta-data plays an important role. On the spatial side it lets us identify which images or polydata can be analyzed together. In other words, it is through this metadata that it becomes to possible to identify matching pieces of information belonging to different subjects. In the case of variables, the metadata provides important information on how to interpret the values that the variable takes. Some concrete examples of data are
\begin{itemize}
\item Image:
\begin{itemize}
\item T1-weighted structural image
\item Color coded DTI image with 3 scalar values per voxel
\item A label map where each voxel contains a value indicating to what structure it belongs 
\end{itemize}
\item PolyData:
\begin{itemize}
\item A group of fibers from a tratography
\item Cerebral cortex surface reconstruction
\item An iso-surface representing an activated area in a fMRI paradigm
\end{itemize}
\item Variable:
\begin{itemize}
\item Gender
\item Height
\item Score at a neuropsychological test
\item Time it took to complete a marathon
\item Level of pain in a likert scale
\item Place of birth
\item Latency in a TMS test
\end{itemize}
\end{itemize}

%Processing

It can be seen that this simple structure allows us to represent a wide array of data types. Some of this data are raw values that can be found directly, and some are the results of processing steps applied to raw data. As mentioned earlier we don't expect our system to do data processing, but there is a close relationship with these kind of procedures. In fact the system should be able to do additional processing when it is required, but delegating this task to third party tools fit for the job. As seen in figure \ref{fig_workflows} processed data is fed back into the system, and afterwards can be used in the same way as raw data. Some examples of the processing steps that we expect to utilize are

\begin{itemize}
\item Estimating diffusion models (like DTI)
\item Reconstructing tractographies
\item Segmentation of structural images
\item Reconstruction of segmented structures
\item Cortical surface reconstruction and parcellation, as done in FreeSurfer
\item Linear and non linear registration
\item Applying transforms to move spatial objects to a different coordinate system
\item Creating frequency maps from several co-registered label maps.
\item Filtering tractographies
\item Calculating volumes, areas, and mean values of a scalar, for a given surface.
\item Statistical tests involving images from two different groups (like VBM, or second level fMRI analysis)
\item Clustering on a set of variables
\item Fitting of statistical models on a set of variables
\end{itemize}

As usual, this is not an exhaustive nor final list. As the project evolves it is likely that we will have to incorporate additional processing algorithms. The vision is to take advantage of available tools whenever possible. Fortunately there is an ample set of high quality open source tools available as was shown on chapter \ref{chap_related}.

%Stake Holders

The expected users in this project are research groups containing specialists from different backgrounds, however there are several other stakeholders who must be considered in the development of the project. The data comes from real participants, which may have concerns about too many people looking at their data. On the other side the project managers and funding agencies want the best return of investment possible, which means extracting as much information and knowledge as possible from the collected data. Usually the participants of the experiment, whose data was collected, signed a consent which permits some use of the data. The contents and permissions granted by these agreements have to be taken into account. A common trade-off is for the participants to grant unlimited usage of the data as long as it is anonymized. In theory it shouldn't be possible to figure out the true identity of a subject based on anonymized data, but this is hard to guarantee in practice, specially when so much complementary data can be obtained from third parties \footnote{Moreno y las recomendaciones an√≥nimas}. Also the anonymization procedure involves adding noise to the data, which will have an impact on the analysis. When data is analyzed as extensively and deeply as we are proposing, the risk of deanonimyzing the subjects increases. Therefore measures have to be taken into account to increase protection, either by limiting access to the data to a small group of researchers, or increasing the strength of anonymization measures. 

Another possible concern is the loss of statistical rigor. As shown in figure \ref{fig_workflows} traditional research is based on the fact that data is used once for a statistical test. In this case meaningful metrics of significance can be reported. However when data is used more than once we fall into a multiple comparisons problem \footnote{quote}, where the chances of false positives increase and therefore corrections should be applied to significance metrics. Nevertheless the scientific community is concerned that researchers will not apply corrections and report results which are not really accurate. While we can't do anything against dishonest researchers, the proposed platforms may make it easier to cheat in order to get more impressive results. As mentioned on the introduction, this problem arises from publication bias. Under this situation some researchers may believe that the best way to guarantee a publication is by reporting some fantastic significance metrics, and try everything they can to get this values. It is key here to remember that the proposed platform is meant to be used as a mean for generating hypotheses, not to formally proof them. While the analysis carried on in the platform may provide evidence in favor of some hypothesis, it is also highly susceptible to the multiple comparisons problems. Therefore it is necessary to be skeptical about the data, and always validate hypotheses using external datasets.  

Understanding the main users of the application is at the core of the design process. Recall that we are using a user centered design methodology, involving end-users constantly during the design process. Understanding the users is also central for the domain-modeling activity. This includes examining their current workflows and tools is the starting point for this. We accomplished this by visiting several labs and hospitals, and watching several experiments, from data acquisition to data analysis. The knowledge from other works in visual analytics can also be applied here. 

First of all, brain researchers are human. As humans we all have limitations on the amount of information we can keep in memory and our ability to concentrate. The analysis platform should help mitigate this by unloading the memory of the specialist to the screen. All of the required information should be visible or easy to access. In order to avoid distractions, it is important to have a fast response time. If the application constantly makes the user wait, then probably he will loose interest and move to another task. A peculiarity of brain researchers is that they are very busy, and usually there is no time in their agenda reserved for data exploration. This activity is carried on at the spaces between tasks, and even tough there is a high chance of being interrupted by students or colleagues. Therefore it is essential for the platform to support work at different intervals and under constant interruptions. In practice this means efficient facilities to save and restore work, as well as additional information to recover the flow from the last session. 

Visual Analytics recognizes that humans have limited cognitive capacity, and the point is to make the best possible use of it. One key point here is that the cognitive budget should be spent on the data and not on the tools. This means that the experts should always be thinking about the underlying data, not on details of the tool used to look at it. Consider for example watching TV, how much time do you spend thinking on details of the tv-set? If tools require complex commands, or the controls are not intuitive, the attention will have to shift to how to accomplish a task in the tool. Another key concept required here is that tools should adapt to users, instead of making users adapt to tools \autocite{norman_design_2002}. This also means that technical details or repetitive actions should be solved automatically. For example there are dozens of image data formats available, and each tool works with some of them. This forces the user to spend time thinking which tool is appropriate for each format, or if there is a need to convert it. The same goes to dealing with files in file system, users should not be thinking on what is the best way to organize files so that they can be easily found afterwards. 

On the other side, researchers are the most fit, if not the only, who can interpret and make sense of this vast amount of data. As the data grows larger, chances of patterns appearing by chance also increase. Only experts can discriminate between true interesting findings and random noise. They can do this because they possess a strong theoretical background, which can be used to give meaning to data. They also have experience working with patients and data, and therefore can associate data from a study to past experiences and close cases. From all these they have also developed an intuition, that can guide them to interesting places. Given the familiarity with the data, experts can quickly spot common mistakes or find simple explanations to what may appear surprising for an outsider. 

In summary some of the main characteristics of a hypothetical user are

\begin{itemize}
\item Limited memory
\item Limited concentration
\item Busy schedule
\item Not interested in technical details
\item Background knowledge
\item Experience
\item Intuition
\end{itemize}

Groups of experts are also interesting because their expertise are on different areas. Therefore while they are deeply familiarized with some aspects of the data, other aspects are foreign. In a similar way each expert is inclined to analyze data of a particular nature or in a particular way. Some teams are composed of experts located far from each other. Often they don't even share the same mother language. 


% groups

% Existing viualization applications

% Commonalities and differences in these applications

% Domain Terms

% Use cases

%Feature model


%- Define domain
%-- Examples
%-- Main Features
%-- Relationship to other domains
%- Analysis of existing Applications
%-- Commonalities and Differences
%-- From other domains, need to be compatible
%- Analysis of literatures and experts
%- Domain Terms
%- Domain Concepts
%- Variability of domain concepts
%-- feature model in problem space
% trade offs
% analyzis of combinations

\section{Solution Proposal}


\subsection{Platform}

Proposal for Braviz platform

- Data Flow
- Task 
- Use Case
- Deployment


\subsection{Applications Family}

Proposal for Braviz application family

- Feature model
-- problem space
-- solution space

- rationale for each choice
- when to select each feature
- how to choose between alternatives
