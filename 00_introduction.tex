

One of the challenges in brain research is finding relationships between the physical structure of the brain and the way it functions. Structural information is gathered mainly through the use of imaging techniques as Magnetic Resonance Imaging (MRI), Computer Aided Tomography (CAT) or Positron Emission Tomography (PET). Other methods measure the electrical activity of the brain, as in Electro-Encephalography  (EEG) or Magneto-Encephalography (MEG). Transcranial Magnetic Stimulation (TMS) is a technique where cells are stimulated using a rapid changing magnetic field, which in turn generates an electrical field inside the neurons, the effects of this stimulation are afterwards measured elsewhere. 

Traditionally research is done by formulating hypotheses and designing experiments to test such hypotheses. Next, recruiting subjects, performing the experiment on each of them and gathering data. Finally this data would be analyzes using statistical methods which provide evidence in favor or against the hypothesis. This methodology imposes limitations on how data is used. Usually data is only used once, which is a shame because gathering this data is expensive in time, effort and resources. 

In the past years there has been a shift towards gathering data in a more open fashion, and several public databases have appeared. There has been several improvements in the way data is collected, stored and shared, both at the technical level and at the policies level. Even inside small research groups, it has become usual to keep looking at the data after traditional hypothesis testing is complete. 

All of this is leading to a change in the way research is done. This is a shift from hypothesis driven research into data driven research. This also creates an increased need for exploratory analysis methods, where the ecosystem is dominated by methods created for confirmatory analysis. In this context new challenges appear. It is now necessary to manage, analyze and visualize data where the number of subjects is increased by orders of magnitude, as well as the measures available for each one. In this scenario it is hard to guarantee homogeneity on the data belonging to each subject. In several cases there are measures available for each subject at several points in time. 

The work-flow in this kind of analysis differs significantly from the traditional one. It requires iterating trough the data several times, looking at it from different points of view, searching for relevant subjects and measures, gathering details from individuals and performing group analyzes involving several measures. 

Similar scenarios have appeared in other domains, as in economy, terrorism prevention and business intelligence. The challenge is always extracting meaningful information from large and heterogeneous data-sets. Approximations to the problem often involve statistics, machine learning and databases together with efficient and intuitive interfaces and data visualizations. Visual Analytics has emerged as a discipline which attempts to integrate all of these areas with the objective of making an optimal use of the available data. Visual Analytics recognizes the human analyst as the most important element in the task, and focuses on letting the analyst work with freedom and efficiency and focused as much as possible on the data instead of the tools.

In this thesis visual analytics techniques are applied to the particular case of cohort studies in brain data. A model which abstracts and formalizes the elements of this task is proposed. This model can be adapted and applied to other domain where cohort-like data is found. The model is materialized in a software environment called BRAVIZ. This software was successfully used in a large brain study performed by the Kangaroo Foundation. The results are very encouraging and show that the proposed tools really helped the experts feel like they really owned their data and could move trough it freely and explore it as they liked. This created a pleasant experience where information, questions and hypotheses could be gathered from the data.


\section{Visual Analytics}

%Que es

Visual analytics is a discipline which "combines automated analysis techniques with interactive
visualizations for an effective understanding, reasoning and decision making on the basis of very
large and complex datasets" \autocite{cook_illuminating_2005}. It is based on the premise that computers and
human beings have different sets of skills which should complement each other. Modern
computing systems are able to store and operate on very large amounts of data.
Specifically data-mining, clustering, machine learning and complex statistical methods can be
applied to Terabytes of data using high performance computers or clusters. These
algorithms, however, are limited in that they lack the theoretical framework, context and critical
thinking abilities necessary to make sense of data, specially when searching for the unknown. 
On the other hand, domain experts have a very rich theoretical
background, expertise and intuition which allow them to grasp the meaning of data and to
make better decisions about the direction that the analysis should move to. 

In Visual Analytics an environment where humans and machines can work
together and communicate fluidly is proposed, such that the specific abilities of both are used
efficiently. Fluid communication is achieved through rich interactive
visualizations and friendly user interfaces.
The human visual system has a large capacity to process information when it is
represented in the appropriate way \autocite{ware_information_2004}. Data visualization has been studied
exhaustively in the last years and many efficient ways to represent large multidimensional
datasets have been designed \autocite{heer_tour_2010}. As
pointed by Ware: "`The best visualizations are not static images to be printed in books, but fluid,
dynamic artifacts that respond to the need for different views or for more detailed
information"' \autocite{ware_information_2004}, interactivity is a fundamental component of these systems. 
However,  these interactions should be
as simple and intuitive as possible, in order to allow the analysts to focus their attention on the
data itself \autocite{spence_information_2007} rather than on details of the tools. 

In a visual analytics system automatic data processing algorithms are combined with interactive data visualizations
inside a loop \autocite{keim_mastering_2010}. The goal of the loop is to extract knowledge from data. Yet again, this process
is much more efficient when automatic tools are combined with visualizations and therefore allowing the intervention of the
human expert at early stages. 

The benefits of visual analytics are more evident when dealing with large and complex datasets. In this case it is also important
to have an efficient data storage infrastructure which can drive all the interactive process. 

From all of the above it can be seen that visual analytics
has to be supported on several areas of knowledge, such as data mining, data management, perception and cognition, human-computer interaction and scientific and information visualization \autocite{keim_visual_2008}. Elements from all these areas have to be integrated smoothly in order to achieve the goal of keeping the analyst focused on the data. In many cases this means managing in the background details that are not relevant for the task at hand. The analyst is always the main character in visual analytics, and all the other components should serve him, and be adapted to its needs. Therefore studying the user, its way of working, and the particular analysis tasks is a requirement to design this kinds of systems. 
%Ejemplos


These concepts can be applied in several domain as law enforcement, business intelligence, city planning, network analysis and bio-informatics\footnote{Tarea: Buscar referencias}. In fact the framework can be applied everywhere there is a need to extract meaning from data, specially where data is large, heterogeneus, noisy, incomplete or unstructured. However because it is centered on the domain, and particular analysis needs, there can't be a single system that works on every domain. Actually the biggest contribution of visual analytics is recognizing the analyst as the most important element, and putting all these disciplines at his service in order to help him solve his analysis tasks. The analysts is always the one in control and the one who knows what is important and what isn't. The fact that the system should adapt to the system, and not the other way around \autocite{norman_design_2002} absolutely holds. 

\section{User Centered Design}

Designing systems that truly meet users' needs requires taking the user into account from the start of the design process. This may sound obvious and easy, but that is not the case. There are several examples of systems that are outstanding from a technical level but fail to be useful. User centered design\footnote{alguna cita importante}, proposes a methodology to efficiently consider the user as part of the design process. The methodology proposes ways to model user needs, analysis tasks, and use cases. Users should constantly be involved. The process is composed of several design, implementation and evaluation cycles. The first cycles should be short and produce low fidelity prototypes. Early evaluations should involve real users and real applications to the best extent possible in order to detect problems and opportunities.  

\begin{figure}
\includegraphics[scale=1]{figures/espiral2.png} 
\caption{The user centered design process}
\end{figure}

The analysis phase should start from a conscious investigation of user needs. Characterizing the users current work-flow and analysis tasks is a priority. It is also worth analyzing the tools they use and understanding the data and processes involved.

Some people argue that users don't really know what they want. There is some truth in this, because users sometimes can't directly point you to what they want. Sometimes users get so used to going through loops in their daily work, that they start to think of it as a natural step in the process. In order to go through this it is very important to pay attention and to ask questions. Don't take anything for granted. 

For example in one of the labs we visited the researcher was showing us how to analyze some TMS data. One of the steps involving moving the file to the desktop. When asked about why, he told us that the next program would not work otherwise. This was natural for him, but it shouldn't be like that. This is an unnecessary steps that has a negative impact on productivity. 

It is very important to avoid placing the designer ideas into the domain expert's mind. Ideas should come from experts, but sometimes you will get the temptation to propose something. The result is that the expert would almost immediately agree. However it is likely that he agrees just to make the designer happy, and not really thinking deeply on the idea. During the first stages of work with process the designers should limit to listen, observe and ask questions in an open manner.

During the design phase, all the information gathered from the subjects should be sorted and analyzed. Patterns and common needs should be made evident, and decisions should be made based on these observations. Proposals should be materialized in prototypes which will be the prime matter for the evaluation phase.

During evaluation phases domain experts are again the most important asset. They should look at the prototypes thoroughly and evaluate which features are useful and which ones not so much. Evaluation processes are best if they can be done in a realistic scenario, using example data and tasks similar to the ones done on a day to day basis by the experts. If possible, quantitative and qualitative data should be collected. There is a high risk of involuntarily biasing the evaluation process, so measures must be taken to make it as honest and transparent as possible. The output from these evaluations will be the starting material for the next analysis phase. 


\section{Exploratory and Confirmatory Analysis}



Economia, como freakanomics, datos de censos, datos publicos.

Data Driven Research

\section{Exploratory Analysis in Brain Research}


