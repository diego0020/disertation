

One of the challenges in brain research is finding relationships between the physical structure of the brain and the way it functions. Structural information is gathered mainly through the use of imaging techniques as Magnetic Resonance Imaging (MRI), Computer Aided Tomography (CAT) or Positron Emission Tomography (PET). Other methods measure the electrical activity of the brain, as in Electro-Encephalography  (EEG) or Magneto-Encephalography (MEG). Transcranial Magnetic Stimulation (TMS) is a technique where cells are stimulated using a rapid changing magnetic field, which in turn generates an electrical field inside the neurons, the effects of this stimulation are afterwards measured elsewhere. 

Traditionally research is done by formulating hypotheses and designing experiments to test such hypotheses. Next, recruiting subjects, performing the experiment on each of them and gathering data. Finally this data would be analyzes using statistical methods which provide evidence in favor or against the hypothesis. This methodology imposes limitations on how data is used. Usually data is only used once, which is a shame because gathering this data is expensive in time, effort and resources. 

In the past years there has been a shift towards gathering data in a more open fashion, and several public databases have appeared. There has been several improvements in the way data is collected, stored and shared, both at the technical level and at the policies level. Even inside small research groups, it has become usual to keep looking at the data after traditional hypothesis testing is complete. 

All of this is leading to a change in the way research is done. This is a shift from hypothesis driven research into data driven research. This also creates an increased need for exploratory analysis methods, where the ecosystem is dominated by methods created for confirmatory analysis. In this context new challenges appear. It is now necessary to manage, analyze and visualize data where the number of subjects is increased by orders of magnitude, as well as the measures available for each one. In this scenario it is hard to guarantee homogeneity on the data belonging to each subject. In several cases there are measures available for each subject at several points in time. 

The work-flow in this kind of analysis differs significantly from the traditional one. It requires iterating trough the data several times, looking at it from different points of view, searching for relevant subjects and measures, gathering details from individuals and performing group analyzes involving several measures. 

Similar scenarios have appeared in other domains, as in economy, terrorism prevention and business intelligence. The challenge is always extracting meaningful information from large and heterogeneous data-sets. Approximations to the problem often involve statistics, machine learning and databases together with efficient and intuitive interfaces and data visualizations. Visual Analytics has emerged as a discipline which attempts to integrate all of these areas with the objective of making an optimal use of the available data. Visual Analytics recognizes the human analyst as the most important element in the task, and focuses on letting the analyst work with freedom and efficiency and focused as much as possible on the data instead of the tools.

In this thesis visual analytics techniques are applied to the particular case of cohort studies in brain data. A model which abstracts and formalizes the elements of this task is proposed. This model can be adapted and applied to other domain where cohort-like data is found. The model is materialized in a software environment called BRAVIZ. This software was successfully used in a large brain study performed by the Kangaroo Foundation. The results are very encouraging and show that the proposed tools really helped the experts feel like they really owned their data and could move trough it freely and explore it as they liked. This created a pleasant experience where information, questions and hypotheses could be gathered from the data.


\section{Visual Analytics}

%Que es

Visual analytics is a discipline which "combines automated analysis techniques with interactive
visualizations for an effective understanding, reasoning and decision making on the basis of very
large and complex datasets" \autocite{cook_illuminating_2005}. It is based on the premise that computers and
human beings have different sets of skills which should complement each other. Modern
computing systems are able to store and operate on very large amounts of data.
Specifically data-mining, clustering, machine learning and complex statistical methods can be
applied to Terabytes of data using high performance computers or clusters. These
algorithms, however, are limited in that they lack the theoretical framework, context and critical
thinking abilities necessary to make sense of data, specially when searching for the unknown. 
On the other hand, domain experts have a very rich theoretical
background, expertise and intuition which allow them to grasp the meaning of data and to
make better decisions about the direction that the analysis should move to. 

In Visual Analytics an environment where humans and machines can work
together and communicate fluidly is proposed, such that the specific abilities of both are used
efficiently. Fluid communication is achieved through rich interactive
visualizations and friendly user interfaces.
The human visual system has a large capacity to process information when it is
represented in the appropriate way \autocite{ware_information_2004}. Data visualization has been studied
exhaustively in the last years and many efficient ways to represent large multidimensional
datasets have been designed \autocite{heer_tour_2010}. As
pointed by Ware: "`The best visualizations are not static images to be printed in books, but fluid,
dynamic artifacts that respond to the need for different views or for more detailed
information"' \autocite{ware_information_2004}, interactivity is a fundamental component of these systems. 
However,  these interactions should be
as simple and intuitive as possible, in order to allow the analysts to focus their attention on the
data itself \autocite{spence_information_2007} rather than on details of the tools. 




%Ejemplos

Visual analytics is a discipline created for the analysis of large unstructured datasets in different domains. In visual analytics the power of modern computers is combined with the human experts’ visual analysis capacity and creative thinking in order to solve complex problems based on complex data. We believe these principles can be applied to brain research allowing domain experts to manipulate and explore data in a way that is not possible with traditional tools. Recent advances in neuroimaging methods have increased the complexity and quantity of data that researchers have to deal with, and commonly, only a fraction of the potential information is extracted.

Visual analytics’ main difference with traditional computational tools is that it recognizes domain experts as the most powerful actors in data analysis. Fully automatic tools, like machine learning or data mining systems, can handle enormous amounts of data but they are not able to make data meaningful. Only domain experts can interpret data and transform it into knowledge and decisions. Domain experts can also quickly spot errors or strange behaviors given their experience with the data and their true understanding of it. Furthermore experts possess intuition and creative thinking which can make the analysis move in unexpected directions.


Economia, como freakanomics, datos de censos, datos publicos.

\section{Exploratory and Confirmatory}

Data Driven Research

\section{Exploratory in Brain Research}

\section{User Centered Design}
