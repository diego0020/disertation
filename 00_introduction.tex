

One of the challenges in brain research is finding relationships between the physical structure of the brain and the way it functions. Structural information is gathered mainly through the use of imaging techniques as Magnetic Resonance Imaging (MRI), Computer Aided Tomography (CAT) or Positron Emission Tomography (PET). Other methods measure the electrical activity of the brain, as in Electro-Encephalography  (EEG) or Magneto-Encephalography (MEG). Transcranial Magnetic Stimulation (TMS) is a technique where cells are stimulated using a rapid changing magnetic field, which in turn generates an electrical field inside the neurons, the effects of this stimulation are afterwards measured elsewhere. 

Traditionally research is done by formulating hypotheses and designing experiments to test such hypotheses. Next, recruiting subjects, performing the experiment on each of them and gathering data. Finally this data would be analyzes using statistical methods which provide evidence in favor or against the hypothesis. This methodology imposes limitations on how data is used. Usually data is only used once, which is a shame because gathering this data is expensive in time, effort and resources. 

In the past years there has been a shift towards gathering data in a more open fashion, and several public databases have appeared. There has been several improvements in the way data is collected, stored and shared, both at the technical level and at the policies level. Even inside small research groups, it has become usual to keep looking at the data after traditional hypothesis testing is complete. 

All of this is leading to a change in the way research is done. This is a shift from hypothesis driven research into data driven research. This also creates an increased need for exploratory analysis methods, where the ecosystem is dominated by methods created for confirmatory analysis. In this context new challenges appear. It is now necessary to manage, analyze and visualize data where the number of subjects is increased by orders of magnitude, as well as the measures available for each one. In this scenario it is hard to guarantee homogeneity on the data belonging to each subject. In several cases there are measures available for each subject at several points in time. 

The work-flow in this kind of analysis differs significantly from the traditional one. It requires iterating trough the data several times, looking at it from different points of view, searching for relevant subjects and measures, gathering details from individuals and performing group analyzes involving several measures. 

Similar scenarios have appeared in other domains, as in economy, terrorism prevention and business intelligence. The challenge is always extracting meaningful information from large and heterogeneous data-sets. Approximations to the problem often involve statistics, machine learning and databases together with efficient and intuitive interfaces and data visualizations. Visual Analytics has emerged as a discipline which attempts to integrate all of these areas with the objective of making an optimal use of the available data. Visual Analytics recognizes the human analyst as the most important element in the task, and focuses on letting the analyst work with freedom and efficiency and focused as much as possible on the data instead of the tools.

In this thesis visual analytics techniques are applied to the particular case of cohort studies in brain data. A model which abstracts and formalizes the elements of this task is proposed. This model can be adapted and applied to other domain where cohort-like data is found. The model is materialized in a software environment called BRAVIZ. This software was successfully used in a large brain study performed by the Kangaroo Foundation. The results are very encouraging and show that the proposed tools really helped the experts feel like they really owned their data and could move trough it freely and explore it as they liked. This created a pleasant experience where information, questions and hypotheses could be gathered from the data.


\section{Visual Analytics}

%Que es

Visual analytics is a discipline which "combines automated analysis techniques with interactive
visualizations for an effective understanding, reasoning and decision making on the basis of very
large and complex datasets" \autocite{cook_illuminating_2005}. It is based on the premise that computers and
human beings have different sets of skills which should complement each other. Modern
computing systems are able to store and operate on very large amounts of data.
Specifically data-mining, clustering, machine learning and complex statistical methods can be
applied to Terabytes of data using high performance computers or clusters. These
algorithms, however, are limited in that they lack the theoretical framework, context and critical
thinking abilities necessary to make sense of data, specially when searching for the unknown. 
On the other hand, domain experts have a very rich theoretical
background, expertise and intuition which allow them to grasp the meaning of data and to
make better decisions about the direction that the analysis should move to. 

In Visual Analytics an environment where humans and machines can work
together and communicate fluidly is proposed, such that the specific abilities of both are used
efficiently. Fluid communication is achieved through rich interactive
visualizations and friendly user interfaces.
The human visual system has a large capacity to process information when it is
represented in the appropriate way \autocite{ware_information_2004}. Data visualization has been studied
exhaustively in the last years and many efficient ways to represent large multidimensional
datasets have been designed \autocite{heer_tour_2010}. As
pointed by Ware: "`The best visualizations are not static images to be printed in books, but fluid,
dynamic artifacts that respond to the need for different views or for more detailed
information"' \autocite{ware_information_2004}, interactivity is a fundamental component of these systems. 
However,  these interactions should be
as simple and intuitive as possible, in order to allow the analysts to focus their attention on the
data itself \autocite{spence_information_2007} rather than on details of the tools. 

In a visual analytics system automatic data processing algorithms are combined with interactive data visualizations
inside a loop \autocite{keim_mastering_2010}. The goal of the loop is to extract knowledge from data. Yet again, this process
is much more efficient when automatic tools are combined with visualizations and therefore allowing the intervention of the
human expert at early stages. 

The benefits of visual analytics are more evident when dealing with large and complex datasets. In this case it is also important
to have an efficient data storage infrastructure which can drive all the interactive process. 

From all of the above it can be seen that visual analytics
has to be supported on several areas of knowledge, such as data mining, data management, perception and cognition, human-computer interaction and scientific and information visualization \autocite{keim_visual_2008}. Elements from all these areas have to be integrated smoothly in order to achieve the goal of keeping the analyst focused on the data. In many cases this means managing in the background details that are not relevant for the task at hand. The analyst is always the main character in visual analytics, and all the other components should serve him, and be adapted to its needs. Therefore studying the user, its way of working, and the particular analysis tasks is a requirement to design this kinds of systems. 
%Ejemplos

These techniques have been succesfully applied in many domains. 


\section{Exploratory and Confirmatory}

Economia, como freakanomics, datos de censos, datos publicos.

Data Driven Research

\section{Exploratory in Brain Research}

\section{User Centered Design}
