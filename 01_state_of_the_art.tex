
\section{Visual Analytics}
\begin{itemize}
	\item Illuminating the path
	\item Vismaster (Daniel Kein)	
	\item Ware
	\item D. Cook, Hadley
	\item Tukey
	\item Tufte	
	\item Algo de user centered
\end{itemize}	

The work by Tufte \autocite{tufte_visual_1983} emphasizes that data should be the center of all visualizations, and additional elements should be kept at a minimum. Labels in axes should correspond to the values of the data, instead of arbitrary \emph{round} numbers. Graphical elements and decorations that don't add anything to the understanding of the data (ducks) should be avoided. It is also important to keep in mind how the human brain perceives elements, and therefore avoid unnatural representations. For example, one should not map values to volumes in a paper or a computer screen. He also introduces small multiples displays as a way of efficiently comparing multidimensional data. In this representations full displays of each point are represented are displayed next to each other, all at the same scale and with the same set of visual parameters. This allows users to focus directly on the data, and don't worry about individual representation characteristics of each image.

The visual analytics term was coined in Illuminating the Path \autocite{cook_illuminating_2005}, a report by Jim Cook, where he outlines the importance of these techniques for national safety, and gives recommendations of 	where research should focus. (Ver archivo complementario)
	
\autocite{ware_information_2004} Information visualization. Review of perception issues affecting how images are understood in the brain. It also explains how interaction can be used to think about data. It insists on that "the cognitive impact of the interface should be minimized, so that thinking is about the problem and not the interface". The eye can move jump instantly from one position to another, while moving a mouse requires time. Switching to another windows by moving the eyes is much faster and easy than by using the mouse.
	
\autocite{fayyad_information_2002}	Information visualization in data mining and knowledge discovery (en la biblioteca, pg 9 revisión biblio)


\autocite{spence_information_2007} provides an overview of the challenges and strategies present in interactive visualization settings.

\autocite{keim_visual_2008} "For informed decisions, it is indispensable to include humans in the data analysis process to combine flexibility, creativity, and background knowledge with the enormous storage capacity and the computational power of today’s computers." Includes several examples of VA applications.
	
\autocite{card_structure_1997}	distinguishes between the graphs using for sharing knowledge (publishing) and those used for discovering knowledge (exploration). 

Research on the theoretical aspects of data visualization \autocite{purchase_theoretical_2008} tries to better understand the factors that determine if a visualization is effective, and in this way predict which visualizations would be the most effective for a certain task and user.
	

\autocite{chen_illuminated_2012} shows the impact of Jim Cook book's in the research community during the next years, with a significant number in visual analytics related invistigations.
	
	
\autocite{chul_kwon_visual_2011} presents some common problems new users face with visual analytics systems based on a user study, and propose some mechanisms to mitigate them.
	
A review of common interactive techniques can be found in \autocite{heer_interactive_2012}. It is recommended to provide several linked views of the data, and allow connection between them. Systems should also work at the same speed as human reasoning does (fast). It is important to provide mechanisms to filter data and see more details of interesting data points. It is also convenient to include basic statistical functions to avoid making the user change to a different program, which would cause a disruption on the workflow. Likewise, maintaining a history of the analysis allows specialists to review the steps that lead to a certain point, and to go back and take detours in the data analysis. Having the option to undo all actions allows the specialist to take more risks and therefore move faster in the analysis. This history works better if it can be annotated to reflect what was going on in the specialists mind at different points. These mechanisms can also foster collaboration and sharing of information between members of a group. Communication between members of the group is more efficient if all members have access to the same visualization, even if they are physically at different places.


Examples : 
Document Analysis: Inspire \autocite{hetzler_analysis_2004}, Atlas TI, Theme River \autocite{themerivertm:_2002}
Military: Strategic airlift \autocite{soban_visual_2011},


\section{Analysis and Visualization Tools}

Data acquisition and visualization methods have rapidly evolved in the past years \autocite{botha_individual_2012}.

Se debe hacer una tabla mostrando las características de cada una,
Luego se debe hacer un feature diagram.

Dividir por categorias
- tractografia
- funcional
- formas
- segmentacion
- registro
- visualizacion

\subsection{Analysis and Visualization of Brain Spatial Data}
\begin{itemize}
	\item 3D-Slicer
	\item FreeView (from freesurfer)
	\item FreeSurfer TK view
	\item FSL-view
	\item BrainVisa
	\item Osirix
	\item ParaView
	\item ImageJ
	\item SPM
	\item Matlab
	\item ITK-snap
	\item MRI-cron
	\item SPHARM
	\item Shape analysis	
	\item TBSS
	\item Invizian
	\item Brain Visa
	\item Ants
	\item Nipy
	\item ITK
	\item VTK
	\item Camino
\end{itemize}


Processing:

BrainVoyager \autocite{goebel_brainvoyagerpast_2012} is a commercial package for high performance processing of functional, diffusion and structural brain data. Its algorithms are optimized for producing fast results (some even realtime), by using all of the available hardware. It includes a scripting language for analyzing bulks of images, and it is also incorporating the newest techniques as Multi-Voxel pattern analysis, a machine learning technique for inferring the state of the mind based on data from functional imaging and a previous training. By doing this analysis in real time it can be used for brain computer interfaces. 


Several software packages are available for processing diffusion weighted images \autocite{hasan_review_2011}

Reconstructing fibers that cross each other, or fold in tight angles is a hard problem \autocite{fillard_quantitative_2011}, and specially tensor based methods are not able to deal with them \autocite{tournier_diffusion_2011}.


Fibers:

\autocite{blaas_fast_2005} presents a method for isolating fiber bundles by defining several regions of interest. Internally bundles are organized in a KD-Tree which permits interactive selection.

\autocite{goodlett_group_2008} proposes a method for performing statistical analysis of white matter, by constructing a representative fiber bundle of the population understudy, mapping it back to the diffusion image of each subject and sampling scalar values from it. In this way statistics can be performed along the tract, with scalars taken equivalent locations on each subject.

\autocite{colby_along-tract_2011} proposes analyzing scalar values along the fiber, instead of extracting just one number that represents the whole bundle, this allows finding patterns that relate to specific pieces of a white matter way.

TBSS \autocite{smith_tract-based_2006} is another approach for analysis of diffusion data. Its main characteristics is that images from different subjects are co-registered based on the structure of white matter. 

Clustering:

\autocite{song_zhang_identifying_2008} explore two methods for clustering fiber bundles, in the first one fibers are assigned to the same cluster if they are sufficiently close, while the second one tries to optimize a global measure of separation between clusters.

\autocite{guevara_automatic_2012} presents a method for clustering the main pathways on massive tractography datasets based on an atlas composed of common pathways in a group of subjects, and manually labeled by experts. This atlas can afterwards be used to isolate these bundles on new brains.

Networks:

\autocite{rubinov_complex_2010} presents several methods for building networks from MRI data, and how graph theory metrics can be applied and interpreted in such networks.

\autocite{li_visual_2012} introduces a software for robust construction and analysis of brain networks. In this software local diffusion or functional features are calculated at the position of each node, then the system tries to find the best position for the corresponding node in different subjects by minimizing the differences of these local features.

In \autocite{richiardi_decoding_2011} a system for decoding the state of the brain based on the fMRI connectivity graph at different frequencies.

\autocite{alper_weighted_2013} studies visual techniques for comparing two weighted brain networks. The tool is based on a research of analysis tasks common in brain network research. At the end two visualizations are recommended based on a user study; one of them is very good for small networks while the other performs better in networks with many nodes and edges.

Surfaces:

Invizian \autocite{bowman_query-based_2011} provides a representation of multiple brain scans in a "Feature Similarity Space", where all brain surfaces are located in a 3d space in such a way that \emph{similar} brains are nearby. The similarity metric can be defined used several scalar features calculated from each brain. The system also integrates with ggobi to combine the spatial analysis with interactive analysis of scalar data.

\begin{table}
	\centering
		\begin{tabular}
			
		\end{tabular}
	\label{tab_related_applications}
\end{table}


Segmentation of hippocampus, robust in presence of pathology \autocite{kim_robust_2011}.


\subsection{Statistical Analysis and Visualization}
For statistical data analysis, to which we refer to as variables, the most common applications are
\begin{itemize}
	\item Excel
	\item SPSS
	\item Matlab
	\item R
	\item ggobi
	\item ggplot
	\item Shiny
	\item Deducer
	\item Tableau
	\item SAS
	\item Stata
	\item Prism
	\item AABel
	\item D3
\end{itemize}


\section{Other Similar Tools}

\autocite{bezgin_matching_2009} presents a tool for exploring data of the macaque brain. It uses an onthology and an atlas to query spatial features and connectivity, and connects to the \emph{cocomac} database which contains data from several studies. "Comparison between studies provide more meaningful insights than each study alone".

IRaster \autocite{somerville_iraster:_2010}  is a tool for visualization and analyzis of electrical signals acquired from high density intracraneal micro-electrodes. Integrates several known signal analysis methods with interactive visualizations which supports deciphering several spiking patterns.

\autocite{steenwijk_integrated_2010} presents two integrated tools, one for processing of image data in order to extract features, and one for interactively visualize and explore such features. Visualization is achieved through scatter plots and parallel coordinates plots, using techniques such as brushing and coloring by a different variable.  Given one point in the exploration, the tool allows the user to find the raw data that originated it. This system is presented as a tool for hypothesis generation. 

\section{Brain studies}

\autocite{lenroot_brain_2006} analyzes the development of adolescents brain based on about 4000 scans of 2000 subjects. It makes use of manual and automatic techniques for registration and segmentation.

\autocite{konrad_vbmdti_2012} presents a study that integrates local diffusion and structural features from the Broca area to results in a verbal intelligence test. In their discussion they mention that it is hard to specify the direction of this relation, this is, if a the structural difference causes a difference in performance, or if learning of verbal skill causes a change at the structural level. 