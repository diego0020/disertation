
Structure

Exploratory Analysis + Large datasets  = Visual Analytics
+ Image Brain Data => Visual Analysis of Brain Data

\section{Exploratory Analysis}

%Difference from confirmatory analysis
%We need both
%Hypothesis generation
%Main elements of exploratory analysis
%Visualization plays an important role
%Pertinent today, data explosion, open data
%
%Tukey
%Wickham : let data surprise us
%Human Brain Project
%Connectome
%Open Data
%Hypothesis Generation


Confirmatory statistical tools are useful for looking if data supports a particular a hypothesis or fits to a particular pattern.  During confirmatory analysis researchers look at data in a systematic way in order to find answers to questions raised based on previous knowledge. This methods can answer these questions, but it does not leave any room for surprises or for finding the unexpected. Also, sometimes researchers are faced to data sets from which few is known in advance. Analyzing a data set without a clear question or hypothesis requires a completely different approach. ``Finding the question is often most important than finding the answer''\autocite{tukey_we_1980}.

Exploratory data analysis is a methodology for learning and understanding data without the need for previous hypotheses or questions. It seeks to let data talk, and let the researcher understand what the data is showing about the world. Exploratory analysis techniques include descriptive statistics, but the most productive methods are based on data visualization \autocite{tukey_exploratory_1977}. If information is presented correctly, the human eye can instantaneously perceive patterns, trends, and oddities. However, usually one does not know in advance what is this correct way to represent the data, and therefore usually exploratory analysis requires trying different representations. The results from this kind of analyzes are often new questions and new hypotheses, which should be answered using tools from confirmatory analysis. Therefore both methods complement each other.

It is well known that the amount of data available for analysis is growing every day. Large amounts of data are available through social networks and government open data initiatives. Also several large research efforts are making the acquired data publicly available (For example the alzheimer's disease neuroimage initiative \autocite{jack_alzheimers_2008}, and the human connectome project\autocite{marcus_human_2013}). These data sets provide an opportunity and a need for large scale exploratory analysis. ``Secondary use of large and open data sets provides researcher with an opportunity o address high-impact questions that would otherwise be prohibitively expensive and time consuming'' \autocite{viangteeravat_giving_2014}. At a smaller scale, research groups are now able to acquire more data from more subjects. This data is often used in confirmatory analysis, and afterwards many groups are making the data available for exploratory research. 

\section{Visual Analytics}

%What it is?
%Why?
%Data 
%Data acquisition + transformation
%Humans and computers
%Automatic Analysis and Visual Analysis
%
%Jim Cook
%Keim

Visual Analytics emerged as a response to the challenge of making sense of large amounts of heterogeneous data. The term was coined by Jim Cook, who proposed a research agenda on the field in the book Illuminating the Path \autocite{cook_illuminating_2005}. This discipline is located at an intersection between data management, data mining, scientific and information visualization, perception and cognition, and human computer interaction \autocite{keim_visual_2008}; and its main contribution is that it recognizes human domain experts as the most important actor in extracting meaning from data.
``It is indispensable to include humans in the data analysis process to combine flexibility, creativity, and background knowledge with the enormous storage capacity and the computational power of today’s computers.'' \autocite{keim_visual_2008}

The core of visual analytics is creating environments where human experts can work efficiently with high performance computers.
``The science of visual analytics must be built on a deep understanding of how
people sense, reason, and respond. This understanding is essential if we are to create
tools, systems, and processes that complement the strengths and compensate for the
weaknesses of the human beings involved.'' \autocite{cook_illuminating_2005}

This efficient interaction between human experts and computers is achieved through the use of rich interactive visualizations and fluid interfaces. The human visual system is recognized as the most efficient channel for transferring information from the computer to the expert's mind, and at the same time it is a natural pattern finding machine. Computer systems supporting visual analytics must be able to manipulate large amounts of data rapidly in order to keep up with the human expert. Processes that take a long time should provide intermediate visualizations and give the expert the chance to steer them or cancel them if they notice they are not going in the right direction. Results obtained through expert interaction are immediately understandable, while those obtained by machine learning require an important interpretation effort \ref{stahl_overview_2013}.

Daniel Keim presents the visual analytics process as several iterations of data transformations, automatic data mining, model refinement, data mapping to visual representations, model building, model visualization, and user interaction \autocite{keim_mastering_2010} (see figure \ref{fig_workflows}-b). This thesis is focused on the area of data visualization and user interaction to support visual analytics on data from brain studies. Data transformation and data mining will be done using third party tools.

The discipline of visual analytics is still young, and there are still several challenges that need to be solved, specially towards making it available for the general public\autocite{kwon_visual_2011}. Since the book by Jim Cook there has been significant interest in visual analytics research \autocite{chen_illuminated_2012}, and more tools have become available. Likewise, there has also been important developments on all the areas that support visual analytics. At the same time there is an increased interest from governments and organizations to extract value from data. All these indicated that visual analytics will continue growing and become a big player in business, government and research.


\subsection{Interactive Visualization}

%Difference from standard visualization
%Perception
%Clean / easy to grasp
%Interaction
Interactive visualizations are one of the main supporting elements of visual analytics. Bertin 
\autocite{bertin_graphics_1981} introduces two different types of graphics, ones used for communicating information, and others used for graphical processing. This thesis will be focused on the second type, but notice that Jim Cook recognized information dissemination as an important component in visual analytics \autocite{cook_illuminating_2005}.

In this context graphics are used as a way of thinking about data. 

\autocite{card_structure_1997}	more examples of interactive visualizations. 

In \autocite{ware_information_2004} Colin Ware reviews perception issues affecting how images are understood in the brain. It also explains how interaction can be used to think about data. It insists on that "the cognitive impact of the interface should be minimized, so that thinking is about the problem and not the interface". The eye can move jump instantly from one position to another, while moving a mouse requires time. Switching to another windows by moving the eyes is much faster and easy than by using the mouse.

		
In \autocite{ware_visual_2013}, Ware exposes several design patterns for visualization applications, focusing on the interaction between the computer system and the human cognitive capabilities. He represents the system a machine where objects can be moved from the expert's working memory to the computer memory and viceverza. In a similar way problems can be solved by visual pattern search using the expert's visual system or by using the processor in the computer. Also moving the eye to a different area of the screen (or a different screen) is much faster than changing the visualization in the computer.
The objective is to make the best possible use of this resources for the given application. 
%Some interesting patterns for this thesis are reasoning with hybrid of visual display and mental imaginery, drill down-close with hierarchical application, cognitive reconstruction, pattern integration across views using brushing, and pattern comparisons in large information space. 



Interactive data visualization requires to integrate knowledge from databases, machine learning, and statistics with perceptual psychology and human computer interaction among others \autocite{ward_interactive_2010}.





%\autocite{fayyad_information_2002}	Information visualization in data mining and knowledge discovery (en la biblioteca, pg 9 revisión biblio)

\autocite{spence_information_2007} provides an overview of the challenges and strategies present in interactive visualization settings.
	

Research on the theoretical aspects of data visualization \autocite{purchase_theoretical_2008} tries to better understand the factors that determine if a visualization is effective, and in this way predict which visualizations would be the most effective for a certain task and user.
	

A review of common interactive techniques can be found in \autocite{heer_interactive_2012}. It is recommended to provide several linked views of the data, and allow connection between them. Systems should also work at the same speed as human reasoning does (fast). It is important to provide mechanisms to filter data and see more details of interesting data points. It is also convenient to include basic statistical functions to avoid making the user change to a different program, which would cause a disruption on the workflow. Likewise, maintaining a history of the analysis allows specialists to review the steps that lead to a certain point, and to go back and take detours in the data analysis. Having the option to undo all actions allows the specialist to take more risks and therefore move faster in the analysis. This history works better if it can be annotated to reflect what was going on in the specialists mind at different points. These mechanisms can also foster collaboration and sharing of information between members of a group. Communication between members of the group is more efficient if all members have access to the same visualization, even if they are physically at different places.

A similar review of visualization techniques can be found in \autocite{heer_tour_2010}. It emphasizes that visualizations should consider the natural strengths of the human visual system to see trends, patterns and identify outliers.



\autocite{yi_toward_2007} provides a taxonomy of user intentions when analyzing data trough interactive visualizations, where such systems are characterized by the fact that information from the system to the user is much larger than from the user to the system. The proposed taxonomy seeks to be descriptive (captures most user intentions, and permits easy classification of new techniques), evaluative (can help to analyze if a system is helping the user) and generative (can help in the design of new systems). The intentions included in the are the basis for the enumeration of analysis tasks in chapter \ref{chap_model}.


The work by Tufte \autocite{tufte_visual_1983} emphasizes that data should be the center of all visualizations, and additional elements should be kept at a minimum. Labels in axes should correspond to the values of the data, instead of arbitrary \emph{round} numbers. Graphical elements and decorations that don't add anything to the understanding of the data (ducks) should be avoided. It is also important to keep in mind how the human brain perceives elements, and therefore avoid unnatural representations. For example, one should not map values to volumes in a paper or a computer screen. He also introduces small multiples displays as a way of efficiently comparing multidimensional data. In this representations full displays of each point are represented are displayed next to each other, all at the same scale and with the same set of visual parameters. This allows users to focus directly on the data, and don't worry about individual representation characteristics of each image.


\subsection{Evaluation in Visual Analytics}

Hard problem
Different techniques
Different stages

\autocite{munzner_nested_2009} describes a model of the different levels at which it is possible to make contributions in data visualization, and the correct approaches to validate claims at each level. It goes from low level algorithm design where measuring computational performance is the correct approach, to domain problem characterization where validation consists on observing adoption rates and interviewing target users.

Los del test con Ana
Schneidermann.... long term in depth...

\subsection{Visual Analytics Examples From Other Domains}

Data acquisition and visualization methods have rapidly evolved in the past years \autocite{botha_individual_2012}.

Examples : 
Document Analysis: Inspire \autocite{hetzler_analysis_2004}, Atlas TI, Theme River \autocite{themerivertm:_2002}
Military: Strategic airlift \autocite{soban_visual_2011},
Chemical Industry: \autocite{stahl_overview_2013}
Multi-attribute ranking LineUP \autocite{gratzl_lineup:_2013}
Public Health: \autocite{sedig_challenge_2014}
Science policy: \autocite{mcinerny_information_2014}
Electricity consumption \autocite{janetzko_anomaly_2014}

\section{Tools for Visual Analysis of Tabular Data}

R  / stata  / python
prism / spss / stata
ggobi
ggplot \autocite{wickham_practical_2008}
ggvis
shiny
deducer
tableau
aabel
d3

\begin{table}
	\centering
		\begin{tabular}
			
		\end{tabular}
	\label{tab_related_tabular_applications}
\end{table}

\section{Neuro-Image Analysis Techniques}

Basics of images, voxels, coordinates, scalars


\subsection{Registration}

FSL
ANTs
SPM-Dartel


Comparing two different brains is a challenging task, specially in the cortex where the pattern of folding between two subjects can be substantially different \autocite{toga_new_2002}.

\subsection{Structural Images}

SPM
FreeSurfer

Segmentation

Segmentation of hippocampus, robust in presence of pathology \autocite{kim_robust_2011}.

Shape analysis

SPHARM

\autocite{hermann_visual_2014} propose a system to interactively explore shape variations between a structure from different subjects or at different times. The system provides three different visualizations at different levels of detail. The first view shows the overall pattern across the structure, the second one allows the user to focus on a substructure and see how changes on it are related to the rest of the structure, and the final one lets the user interactively modify the structure by dragging a  point and see how the rest of the structure would be modified. 

\subsection{Functional Magnetic Resonance}

SPM
FSL
AFNI
Handbook of fMRI

BrainVoyager \autocite{goebel_brainvoyagerpast_2012} is a commercial package for high performance processing of functional, diffusion and structural brain data. Its algorithms are optimized for producing fast results (some even realtime), by using all of the available hardware. It includes a scripting language for analyzing bulks of images, and it is also incorporating the newest techniques as Multi-Voxel pattern analysis, a machine learning technique for inferring the state of the mind based on data from functional imaging and a previous training. By doing this analysis in real time it can be used for brain computer interfaces. 

\subsection{Diffusion Weighted Images}

CAMINO
DIFFUSION TOOLKIT
TRACULA



Several software packages are available for processing diffusion weighted images \autocite{hasan_review_2011}

Reconstructing fibers that cross each other, or fold in tight angles is a hard problem \autocite{fillard_quantitative_2011}, and specially tensor based methods are not able to deal with them \autocite{tournier_diffusion_2011}.


\autocite{blaas_fast_2005} presents a method for isolating fiber bundles by defining several regions of interest. Internally bundles are organized in a KD-Tree which permits interactive selection.

\autocite{goodlett_group_2008} proposes a method for performing statistical analysis of white matter, by constructing a representative fiber bundle of the population understudy, mapping it back to the diffusion image of each subject and sampling scalar values from it. In this way statistics can be performed along the tract, with scalars taken equivalent locations on each subject.

\autocite{colby_along-tract_2011} proposes analyzing scalar values along the fiber, instead of extracting just one number that represents the whole bundle, this allows finding patterns that relate to specific pieces of a white matter way.


TBSS \autocite{smith_tract-based_2006} is another approach for analysis of diffusion data. Its main characteristics is that images from different subjects are co-registered based on the structure of white matter. 

\subsubsection{Tractography Clustering}

\autocite{song_zhang_identifying_2008} explore two methods for clustering fiber bundles, in the first one fibers are assigned to the same cluster if they are sufficiently close, while the second one tries to optimize a global measure of separation between clusters.

\autocite{guevara_automatic_2012} presents a method for clustering the main pathways on massive tractography datasets based on an atlas composed of common pathways in a group of subjects, and manually labeled by experts. This atlas can afterwards be used to isolate these bundles on new brains.

\subsubsection{Connectivity Networks}

\autocite{rubinov_complex_2010} presents several methods for building networks from MRI data, and how graph theory metrics can be applied and interpreted in such networks.

\autocite{li_visual_2012} introduces a software for robust construction and analysis of brain networks. In this software local diffusion or functional features are calculated at the position of each node, then the system tries to find the best position for the corresponding node in different subjects by minimizing the differences of these local features.

In \autocite{richiardi_decoding_2011} a system for decoding the state of the brain based on the fMRI connectivity graph at different frequencies.

\autocite{alper_weighted_2013} studies visual techniques for comparing two weighted brain networks. The tool is based on a research of analysis tasks common in brain network research. At the end two visualizations are recommended based on a user study; one of them is very good for small networks while the other performs better in networks with many nodes and edges.

\subsection{Spatial Data Visualization}

3D Slicer
FreeView / TkView
BrainVisa
Osirix / PACS / Proprietary
VTK / PARAVIEW
MRICRON / ITKSNAP



%---------------------------------------------------------------


%-------------------------------------------------------------------
%In \autocite{paus_mapping_2005} a meta study about brain development is presented. It makes the point that by looking at different modalities of information in an integrated way is useful for making better assessments of intersubjects variance. He also mentions the potential from alliances between image experts with social scientists, geneticists and mental health professionals.

%\autocite{lenroot_brain_2006} analyzes the development of adolescents brain based on about 4000 scans of 2000 subjects. It makes use of manual and automatic techniques for registration and segmentation.

%\autocite{konrad_vbmdti_2012} presents a study that integrates local diffusion and structural features from the Broca area to results in a verbal intelligence test. In their discussion they mention that it is hard to specify the direction of this relation, this is, if a the structural difference causes a difference in performance, or if learning of verbal skill causes a change at the structural level. 


\section{Visual Analytics for Brain Images}

\autocite{bezgin_matching_2009} presents a tool for exploring data of the macaque brain. It uses an onthology and an atlas to query spatial features and connectivity, and connects to the \emph{cocomac} database which contains data from several studies. "Comparison between studies provide more meaningful insights than each study alone".

IRaster \autocite{somerville_iraster:_2010}  is a tool for visualization and analyzis of electrical signals acquired from high density intracraneal micro-electrodes. Integrates several known signal analysis methods with interactive visualizations which supports deciphering several spiking patterns.

\autocite{steenwijk_integrated_2010} presents two integrated tools, one for processing of image data in order to extract features, and one for interactively visualize and explore such features. Visualization is achieved through scatter plots and parallel coordinates plots, using techniques such as brushing and coloring by a different variable.  Given one point in the exploration, the tool allows the user to find the raw data that originated it. This system is presented as a tool for hypothesis generation. 

Invizian \autocite{bowman_query-based_2011, bowman_visual_2012, van_horn_graphical_2013} provides a representation of multiple brain scans in a "Feature Similarity Space", where all brain surfaces are located in a 3d space in such a way that \emph{similar} brains are nearby. The similarity metric can be defined used several scalar features calculated from each brain. The system also integrates with ggobi to combine the spatial analysis with interactive analysis of scalar data.

\begin{table}
	\centering
		\begin{tabular}
			
		\end{tabular}
	\label{tab_related_brain_applications}
\end{table}