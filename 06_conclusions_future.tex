%Answer research questions, close the gap, close the introduction
The previous chapters presented a model that allows to design and implement software tools that support exploratory analysis of data from large brain studies with heterogeneous data from multiple subjects. This model was used to implement a set of applications that supported the analysis of data from a large brain study which included several types of data derived from neuro-images as well as clinical, demographic and socio-economic data. Further tests with potential users showed the relevance of this type of tools in today's research. Tools like the ones proposed in this work can change the way in which information is extracted from data and help in the transition towards a more collaborative and open research. 

\section{Effects in research workflow}

%- Explicit contributions to experts (neuroscience, images, research)

%- Subject as a whole
%-- Integrate data
%-- In context

One of the benefits of the proposed approach is that it lets all researchers look at data from different fields. The effect of this is that there is an integrated view of each subject. Data is not considered in isolation, but in the context of other information from the subject, as well as in the context of the sample. This additional context allows experts to make better interpretations of the data they are focused in. Additionally, this allows researcher to raise questions and hypotheses relating the focus and context data. The benefits of integrating all study data in a single place were illustrated by researchers in the Kangaroo Mother Care study (see chapter \ref{chap_kmc400}) as well as physicians and researchers outside the foundation (see chapter \ref{chap_analysis}).\footnote{Help Cyril: Why is it important to look at subjects as a whole?} 

%- Deeper analysis
%-- Ask new questions
%-- complex questions
%- Extract more information from each data-set
By integrating several kinds of data in a single place, it becomes feasible to ask new questions, and explore the answers conveniently. The system goes further as the data-set can be continuously enhanced by adding new data, which increases the space to ask and explore. This new data is generated by transforming existing data or by manually generating new measures and objects derived from it. The final objective would be a system where technical difficulties never get in the way of the researcher's line of thought. By using the proposed system researchers have gone beyond the original questions and hypotheses of the project and are proposing several new questions on the same data-set. There are still moments where technical limitations get on the way of the analysis, but the system should continue growing in order to address these limitations and let researchers ask more complex questions, and make a better use of the collected data.

%- Better understanding through visualizations
In addition to integrating data from multiple domains the system focuses on presenting data visually. As evidenced in the visual analytics' literature, rich interactive graphics are the most efficient way of transferring information from a computer to the expert. In the domain of this study the benefits of visualizations over only numbers are also evident. Visualization of statistical results provide much more information than single numeric statistics. Plots can provide more accurate descriptions of the details of the relationships, as well as making unusual values grab attention. Visualizations have the power to show the unexpected and to surprise the analysts. Interactive visualizations add even more value as they just not show data behavior but gives tool to investigate the probable causes and to get additional information that could provide insight into this behavior. Braviz implements mechanisms to identify individual points on every visualization and look into each of them in detail. This is specially useful in data derived from image data, where anomalies can be explained by characteristics of the images themselves. By looking at the images and spatial structures behind each measure they can be better interpreted and their limitations can be made visible. Displaying all spatial data in the same space can also help researchers find or design better metrics for each question.

%- Cleaner datasets
%- Time efficiency
% -- Do you spend more time looking at data now?
By making anomalous points in the data-set evident, visualizations contribute to cleaning the data-set. These anomalies can be investigated, and if mistakes are found they can be corrected or if this is not possible the point can be excluded from the analysis. As the analysis continues mistakes in the data-set will be less common, and analysts can feel more confident of the quality of the data and the results that can be extracted from it. An additional effect is that the data-set becomes more valuable as time goes by, and therefore researchers are encouraged to come back to a data-set. An objective of the project was increasing the amount of information that can be extracted from each data-set, and that data collected for a certain study can outlive it and be used in the future. Having a data-set that is not static, but that is continuously cleaned and enhanced with new and transformed data is a key step towards this objective.

%- Encourage exploratory Analysis
%- Share work across experts
%-- more communication
%- Freely explore, follow curiosity, leave room for surprises
It can also be seen that the proposed system supports and encourages exploratory analyzes. By making the complete data-set available to every researcher it also encourages interdisciplinary work. Having access to data from different fields permits experts explore data that was usually out of their reach. This causes an increase in curiosity and a desire to learn more, which in turns promotes asking questions to other experts with more experience in the given data. Also, it is easier for each expert to use visualizations to support and explain their ideas to the rest of the group. This visualizations can be shared and revisited independently by each expert on their own data, which further supports teamwork. Each look at the data opens the door for finding the unexpected, which can lead to new discoveries. 

%- based on the case studies and evidence

\section{Importance of the model}

%How the model helps
%- Support exploratory research workflow
%- More efficient use of developers and experts time
%- Improving data sharing
%- Improving collaboration

Chapters \ref{chap_kmc400} and \ref{chap_analysis} showed that Braviz adapts to experts' workflow and that it is a valuable tool for exploratory analysis. Some of the applications that make up Braviz are more useful to some experts, and others are more used by other experts. But it has been shown that the applications complement each other and that they can be used together during the analysis. The reason why several applications can work as a coordinated system, is that they are all based on the model described in chapter \ref{chap_model}. The model recognizes the fact that there should be specific applications for specific tasks. Each application should be simple to use, but several applications can be used at different stages of a more complex task. The model also encourages implementing features that will make the applications appropriates for work inside the integrated environment. Punctually applications should keep a record of important user actions, they should communicate with other applications in the system and allow the user to save and restore the work at any point. 

Each application is focused on an specific analysis task, therefore the interface is designed to explicitly support this task and don't overwhelm the user with options. These small applications are straightforward to use, and thus users don't need extensive training to start to work. The result is that researchers can be very efficient at these tasks. The user centered design method is used to detect the tasks that are usually bottlenecks in the expert's workflow and therefore the end result is an overall increase in efficiency at exploratory data analysis. 

Even though each application is different, they all share a common base. This allows applications to share data between each other. Several users of the system can therefore collaborate by using data or objects created by each other in any of the applications that make up the system. 

%- Adapt to different users / projects / scenarios
%- Understanding of the design space
%- Thinking of possible solutions
The feature model in problem space (Figure \ref{fig_feature_problem}), which is the basis for each implementation, allow designers to effectively assess the necessities of each user and translate them into a configuration for a new application. In this way the system can be adapted to the needs of different users in a short amount of time. Additionally the architecture of the system with an unique point for reading operations lets developers adjust the whole system to different data formats and layouts only by modifying this single module. 

%- Faster coherent implementation, better maintainability
%- Development time, time to market
%- Solve/maintain common problems only once 
Concrete applications are configured using the feature model from Figure \ref{fig_feature_solution}. This model lets designer discover which components can be reused and which ones need to be added. It also guarantees that that the new application will adapt well to the other applications that are part of the system. By reusing components across the different applications, difficult technical challenges can be addressed in detail a single time, and the whole set of applications will benefit from the changes. Developers can trust in these components, and focus their efforts on designing the visualization and the user interfaces, which are the parts of the application that make the most important difference for end users. Recall that the focus of the system is providing an efficient channel for users to work together with data and processing algorithms in a computer, following the visual analytics principles. Therefore designers should reuse third party processing algorithms, both for statistical and spatial data, whenever this is possible. By using these techniques the developing cycles are reduced and therefore there can be more interaction rounds with end-users which will lead to a design that better fits their needs.

%- Based on Braviz

\section{Future Work}

%- Evaluation
This work presented evidence of the benefits of a visual analytics system in brain research, as well as the benefits of a domain model in the development of such system. However there is still need for additional evaluation in order to better assess the strengths and limitations of the proposal. Evaluating visual analytics systems is a difficult problem (\autocite{cook_illuminating_2005, shneiderman_strategies_2006,lam_seven_2011} ). Controlled laboratory experiments can be used to test concrete details of a visualization, but testing the whole system requires long term experiments which were out of the scope of this project. However it will be interesting to acquire this kind of evidence in the future in order to continue improving the application.

%- Better understanding of thought processes
	%-- Of experts workflow
	%-- Of collaborative research
	%-- Of extracting value from data
Visual analytics application rely on understanding human cognition and sense making. The current prototype integrates several of the recommendations from experts in visual perception, but it can still benefit from a better understanding of the sense making processes that go on in the experts' brain during exploratory research. As the understanding of these processes improve, it should be integrated into the model and used to further increase the efficiency of researchers.

%Challenges
%- Software
%-- Integrating more data
%-- More analyzes
%-- More statistics

The proposed software could also benefit from additional data types and corresponding visualization. In particular there is interest in integrating EEG and MEG data. In the past years analyzing fMRI data as networks has raised in popularity, specially when resting state fMRI is considered. Therefore there is an opportunity available for integrating this kind of analyzis in the system. Additionally there is a need to support comparisons and analyzes involving groups of subjects, for example second level fMRI. Several users have also requested additional statistical tests, specifically non parametric tests, analysis of ordinal variables and multiple measures ANOVA. All of these should be analyzed and designed carefully in order to keep the system easy to use. Probably different projects will need only as subset of the available applications, which would provide an additional level of configuration.
%-- Additional tests
%-- Web,
%-- Cloud,
Web based visualizations are becoming the norm for data analysis. This has the advantage that it is possible to reach directly a wide array of users using different devices. No installations are required, and there is no need to do extensive modifications to support phones, tablets of different operating systems. These applications can run in any modern web browser. However there are also benefits of having native applications, as a more direct control of resources, more options to interact and arguably better performance. Nevertheless it is worth exploring if it is worth migrating to a full web environment, and re-evaluate this decisions as web technology advances.

The current architecture has data, communications, processing and logging running on the client machine. This machine also runs a web server to which other devices can communicate, but most calculations are still performed on the server. Another approach would be to move data and processing to a cloud, and have users run on their machines only the front-end (web or native). This would be simpler for users as they would not require large machines and they could work from anywhere, but it would require a large investment on the other side. The storage and processing nodes on the cloud would also need to be adjusted depending on the number of users. One possibility would be to implement the system on a on-demand cloud server as Google cloud, Amazon or Open Shaft. In this case it would also be required to deal with privacy and security issues. It is necessary to take a careful look at this proposal and decide if it is convenient or not.

%-- Massification
%-- Maintainance
%-- Model evolution
%-- Testing
Right now our proposal is been actively used by a small community. This group of users need to grow in the future, which requires investment on communication, and support. The installation process and documentation require significant improvements. There is also a need to provide support to new users and provide fixes to issues that arise with a larger user base. Robust mechanisms for evolving the system without breaking it for existing users should also be implemented. A crucial part of this would be to have a good set of automatic tests. It would also be desired to have several developers contributing in the future, but this also has to be organized and managed.

%- Data sharing, open science
% -policy
The value of solutions for exploratory analysis, such as the one presented in this project, will increase as more data becomes publicly available. However this requires important changes in the culture and policy behind research. The current system is highly competitive and does not encourage collaboration between teams. Also there are not many incentives for exploring old data-sets, repeating experiments with new data or sharing data and methods with others. 

%- Clinical?
Another questions that needs to be addressed in the future is if there are clinical applications of this proposal. If so, which ones are there, and what needs to be done in order to support them.
%- Other domains
%- Health, Cities, Industry, Economics
Finally, it should be explored if the proposal of multiple small applications with a large common base can be applied to visual analysis in other domains. Some examples could be analysis of transactions in banking systems, city planning, or public health.  